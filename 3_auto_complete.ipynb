{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "*how to build an auto-complete using a pre-trained language model*\n",
        "\n",
        "Example auto-complete: \"I confirm the appointment for tomorrow morning. Best\" -> \"regards,\"\n",
        "\n",
        "Goal\n",
        "\n",
        "How to \n",
        "\n",
        "*   use pre-trained large language models (GPT from OpenAI)\n",
        "*   generate multiple completions\n",
        "*   score likelihood of candidate completions\n",
        "\n",
        "\n",
        "Sections:\n",
        "\n",
        "**Open-ended completions**: how to generate completions\n",
        "\n",
        "**Sub-word tokenizers**: how input and output is split in sub-words\n",
        "\n",
        "**Structured completions (classification)**: given a finite set of candidate completions, how to pick best one"
      ],
      "metadata": {
        "id": "OiyrX3KjmCiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open-ended completions\n",
        "\n",
        "Go to [platform.openai.com](https://platform.openai.com), create a (free) account:\n",
        "\n",
        "In the top right corner, click in your profile and select \"View API keys\" to generate a new key:"
      ],
      "metadata": {
        "id": "hEE8FJSFo01v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0MUmdCtcED2"
      },
      "outputs": [],
      "source": [
        "# insert your api key\n",
        "!pip install openai\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "openai.api_key = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A language model trained on a diverse dataset can complete arbitrary sentences:"
      ],
      "metadata": {
        "id": "MCf6b2eqpY0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"Q: What is the capital of Ontario? A:\""
      ],
      "metadata": {
        "id": "XKt8GrM3PcJo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.Completion.create(model=\"text-davinci-003\", \n",
        "                         prompt=prompt, \n",
        "                         temperature=0, \n",
        "                         max_tokens=20).choices[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1k8bM46wPSb2",
        "outputId": "544fd16f-b906-4d1f-ea4a-8c4ea89f4283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The capital of Ontario is Toronto.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The completions may need to be parsed. \n",
        "Example: extract the city from the answer."
      ],
      "metadata": {
        "id": "K0ixR3yhpqDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For creating an auto complete, we can simply extract the first word from the completion. But most of the work is in parsing the output and deciding when to suggest an auto-complete."
      ],
      "metadata": {
        "id": "rWGaMbZ7p8mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_autocomplete_prompt = \"I confirm the appointment for tomorrow morning.\\nBest\"\n",
        "openai.Completion.create(model=\"text-davinci-003\", \n",
        "                         prompt=word_autocomplete_prompt, \n",
        "                         temperature=0, \n",
        "                         max_tokens=20).choices[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NFgW194bp437",
        "outputId": "c0b2b59c-6ede-4616-a3b3-3e076fdd913b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "',\\n[Your Name]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also can create multiple deiverse completions by setting the temperature higher than 0 and setting the number of outputs to be greater than 1. "
      ],
      "metadata": {
        "id": "ulZa2JH2rCfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completions = openai.Completion.create(model=\"text-davinci-003\", \n",
        "                         prompt=word_autocomplete_prompt, \n",
        "                         temperature=0.5, \n",
        "                         max_tokens=20,\n",
        "                         n=10)\n",
        "for i, c in enumerate(completions.choices):\n",
        "  print(f\"Completion {i}:\\n\\\"{c.text}\\\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyx__6Fhq_IL",
        "outputId": "5bcade9a-0bdd-40c5-d095-a2d2943bbcdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion 0:\n",
            "\",\n",
            "[Name]\"\n",
            "\n",
            "Completion 1:\n",
            "\",\n",
            "[Your Name]\"\n",
            "\n",
            "Completion 2:\n",
            "\",\n",
            "[Your Name]\"\n",
            "\n",
            "Completion 3:\n",
            "\" regards\n",
            "[Your Name]\"\n",
            "\n",
            "Completion 4:\n",
            "\",\n",
            "[Your Name]\"\n",
            "\n",
            "Completion 5:\n",
            "\",\n",
            "[Your Name]\"\n",
            "\n",
            "Completion 6:\n",
            "\" regards\n",
            "[Your name]\"\n",
            "\n",
            "Completion 7:\n",
            "\",\n",
            "[Your Name]\"\n",
            "\n",
            "Completion 8:\n",
            "\",\n",
            "[Your Name]\"\n",
            "\n",
            "Completion 9:\n",
            "\",\n",
            "[Your Name]\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sub-words tokenizers\n",
        "\n",
        "To decide when to give a suggestion, we first need to understand how completions are generated. \n",
        "\n",
        "In the previous lesson, we learned that we can generate outputs character by character. Let's compare three tokenizer approaches:\n",
        "\n",
        "**Character**\n",
        "\n",
        "Draw-backs: \n",
        "- absence of punctuation, upper case... This can be improved by adding all the missing characters to the vocabulary. \n",
        "- lack of higher-level meaning of each token compared to a word level vocabulary.\n",
        "\n",
        "Advantage:\n",
        "- if all characters are included in the vocabulary, it can cover all possible inputs.\n",
        "\n",
        "**Word**\n",
        "\n",
        "Draw-backs: \n",
        "- can be very large \n",
        "- cannot possibily cover all words: new words, names, scientific. So it is very hard to cover every potential input (also inputs may be be simply mispelled).\n",
        "- words sith similar meanings/spellings have different tokens (ex: play/played)\n",
        "\n",
        "Advantage:\n",
        "- each token represents a word/concept, so model can focus on learning relations between words\n",
        "\n",
        "**Sub-word**\n",
        "\n",
        "Trade-off between previous 2. It has the most common words (and word pieces) as well as the character vocabulary, so if an unknown word appears it can generate it from a combination of characters/subwords. Ex: 'play' and 'ed' may be different tokens and 'played' simply the combination of both. Thus, words with the same prefixes may share tokens, so it is easier to generalize concepts to derivative words.\n",
        "\n",
        "# Number of tokens\n",
        "\n",
        "Important because models have limit size measured in:\n",
        "\n",
        "number of input tokens + number of output tokens < max number of total tokens\n",
        "\n",
        "Ex: some OpenAI models have 4000 token limit\n",
        "\n",
        "Also, API calls may be priced by number of tokens.\n",
        "\n",
        "So knowing how many tokens an input has is very useful."
      ],
      "metadata": {
        "id": "zhf635X1scju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "id": "ysZSHPgvRvMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization example:"
      ],
      "metadata": {
        "id": "MO9La1IGx5X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretiffy_tokenizer(tokenized_list):\n",
        "  return [t.replace(\"Ä \", \" \") for t in tokenized_list]\n",
        "\n",
        "tokenized = tokenizer(prompt)['input_ids']\n",
        "print(f\"The tokenized version of the sentence \\\"{prompt}\\\" i s: \\n{tokenized}\")\n",
        "print(f\"The individual tokens are:\\n{pretiffy_tokenizer(tokenizer.convert_ids_to_tokens(tokenized))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptik5ggESHaf",
        "outputId": "89279c62-40ce-4fbd-eab8-88ab2c35490f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokenized version of the sentence \"Q: What is the capital of Ontario? A:\" i s: \n",
            "[48, 25, 1867, 318, 262, 3139, 286, 10553, 30, 317, 25]\n",
            "The individual tokens are:\n",
            "['Q', ':', ' What', ' is', ' the', ' capital', ' of', ' Ontario', '?', ' A', ':']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The vocabulary size is: {len(tokenizer.get_vocab())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONpbvm0XXOqh",
        "outputId": "512e460b-90a8-4573-ecb9-5df200ba3373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size is: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte pair encoding (BPE)\n",
        "\n",
        "https://huggingface.co/course/chapter6/5\n"
      ],
      "metadata": {
        "id": "CxG8sqcMYWXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example with a long work being split into multiple tokens. And also how numbers are also split."
      ],
      "metadata": {
        "id": "f1d0klhHx_aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complex_prompt = \"The desalination rate is 300000 m3/day.\"\n",
        "tokenized = tokenizer(complex_prompt)['input_ids']\n",
        "print(f\"The tokenized version of the sentence \\\"{complex_prompt}\\\" i s: \\n{tokenized}\")\n",
        "print(f\"The individual tokens are:\\n{pretiffy_tokenizer(tokenizer.convert_ids_to_tokens(tokenized))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIqNAIk7SLFY",
        "outputId": "c55b874f-d72b-463d-897e-33055711e88b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokenized version of the sentence \"The desalination rate is 300000 m3/day.\" i s: \n",
            "[464, 748, 282, 1883, 2494, 318, 5867, 830, 285, 18, 14, 820, 13]\n",
            "The individual tokens are:\n",
            "['The', ' des', 'al', 'ination', ' rate', ' is', ' 300', '000', ' m', '3', '/', 'day', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How words with similar meaning share tokens."
      ],
      "metadata": {
        "id": "SK5vla4WzhHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "derivative_words = \"words: hallucinate, hallucinating, hallucination\"\n",
        "tokenized = tokenizer(derivative_words)['input_ids']\n",
        "print(f\"The tokenized version of the sentence \\\"{derivative_words}\\\" i s: \\n{tokenized}\")\n",
        "print(f\"The individual tokens are:\\n{pretiffy_tokenizer(tokenizer.convert_ids_to_tokens(tokenized))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5cU6WUByeyf",
        "outputId": "e172b563-f259-4983-e80f-b3072e14f466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokenized version of the sentence \"words: hallucinate, hallucinating, hallucination\" i s: \n",
            "[10879, 25, 23251, 4559, 11, 23251, 6010, 11, 23251, 1883]\n",
            "The individual tokens are:\n",
            "['words', ':', ' halluc', 'inate', ',', ' halluc', 'inating', ',', ' halluc', 'ination']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured completions\n",
        "\n",
        "To generate structures completions we can:\n",
        "\n",
        "- parse the model output\n",
        "\n",
        "- give the model multiple completions and select the one with highest score"
      ],
      "metadata": {
        "id": "YIjZCyGwzrPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = openai.Completion.create(\n",
        "    model=\"text-davinci-003\", \n",
        "    prompt=prompt, \n",
        "    temperature=0, \n",
        "    max_tokens=0,\n",
        "    echo=True, # return the input\n",
        "    logprobs=1 # return the log-probability\n",
        "    )"
      ],
      "metadata": {
        "id": "_WLh39d4Y2rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See what was the score for each token in the question."
      ],
      "metadata": {
        "id": "aMIzdeDr0GrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token, logprob in zip(completion.choices[0].logprobs.tokens, completion.choices[0].logprobs.token_logprobs):\n",
        "  print((token, logprob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJSELjGHcEHg",
        "outputId": "fa0f427e-c2c2-4e63-b59d-2b6b11b8ab16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Q', None)\n",
            "(':', -2.7656417)\n",
            "(' What', -3.0039053)\n",
            "(' is', -0.84649307)\n",
            "(' the', -0.40047067)\n",
            "(' capital', -7.1475334)\n",
            "(' of', -0.09650371)\n",
            "(' Ontario', -7.120254)\n",
            "('?', -0.53915656)\n",
            "(' A', -9.85958)\n",
            "(':', -0.0041429596)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And also, the next token with highest probability at each generation step.\n",
        "\n",
        "Example: After \"Q:\" the most probable token is \"How\", not \"What\"."
      ],
      "metadata": {
        "id": "xIFDjxax0Q22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in completion.choices[0].logprobs.top_logprobs:\n",
        "  if token:\n",
        "    print(token.to_dict())\n",
        "  else:\n",
        "    print(None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D35IVmPzattF",
        "outputId": "6f419c4b-26a4-4656-c5b2-0d9887d2feb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "{':': -2.7656417}\n",
            "{' How': -2.059068}\n",
            "{' is': -0.84649307}\n",
            "{' the': -0.40047067}\n",
            "{' difference': -1.3592246}\n",
            "{' of': -0.09650371}\n",
            "{' the': -2.6134484}\n",
            "{'?': -0.53915656}\n",
            "{'\\n': -0.009496838}\n",
            "{':': -0.0041429596}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the most probable output by giving a list of candidates to be scores."
      ],
      "metadata": {
        "id": "ikVIZ0sJ0jv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_answers = ['Ottawa', 'Toronto', 'Vancouver', 'Montreal']\n",
        "logprobs = []\n",
        "\n",
        "for c_a in candidate_answers:\n",
        "  raw_logprobs = openai.Completion.create(\n",
        "      model=\"text-davinci-003\", \n",
        "      prompt=' '.join([prompt, c_a]), \n",
        "      temperature=0, \n",
        "      max_tokens=0,\n",
        "      echo=True, \n",
        "      logprobs=1).choices[0].logprobs.token_logprobs[1:]\n",
        "  sumlogprob = sum(raw_logprobs)\n",
        "  avglogprob = sumlogprob/len(raw_logprobs) \n",
        "  logprobs.append((c_a, avglogprob))\n",
        "  "
      ],
      "metadata": {
        "id": "3Rh_z6OQeJCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"For the question: \\\"{prompt}\\\"\\nThe candidate answers and log-probs are:\\n\")\n",
        "for l in logprobs:\n",
        "  print(f\"Answer: {l[0]}, Avg. logprob: {l[1]}\")\n",
        "logprobs.sort(key= lambda x: x[1])\n",
        "print(f\"\\nThe most probable answer is: {logprobs[-1][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U44czWJngiiB",
        "outputId": "5eb704ad-39e0-4a21-cbf0-a39a3be4a3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the question: \"Q: What is the capital of Ontario? A:\"\n",
            "The candidate answers and log-probs are:\n",
            "\n",
            "Answer: Vancouver, Avg. logprob: -4.668726059518182\n",
            "Answer: Montreal, Avg. logprob: -4.434301600709091\n",
            "Answer: Ottawa, Avg. logprob: -3.475510923154545\n",
            "Answer: Toronto, Avg. logprob: -3.2564251196363627\n",
            "\n",
            "The most probable answer is: Toronto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scores can be also useful for open-ended completions. For example we can recommend a completion only if it is above a given score (log-probability)."
      ],
      "metadata": {
        "id": "TDbHA5ce0vex"
      }
    }
  ]
}